{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of Aagar Plates\n",
    "\n",
    "This Jupyter Notebook aims to buil a binary classifier for agar plates. The goal is to classify whether an agar plate contains colonies or not.\n",
    "\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "The dataset used in this notebook consists of 13469 images of agar plates, along with corresponding metadata (json) indicating the number of colonies present on each plate and type of bacteria. The training data is split into a training (80%) set and a validation set (20%).\n",
    "\n",
    "**Model Architecture**\n",
    "\n",
    "The binary classifier model used in this notebook is based on the DenseNet-121 architecture. It uses the pretrained Imagenet weights. The last layer of the model is modified for binary classification.\n",
    "\n",
    "**Training and Evaluation**\n",
    "\n",
    "The model is trained using the training set and evaluated using the validation set. The training process includes data augmentation and transformation techniques to improve the model's performance.\n",
    "\n",
    "**Results and Analysis**\n",
    "\n",
    "After training the model, the notebook provides evaluation metrics such as accuracy and recall. Additionally, it includes visualizations of the model's performance, such as confusion matrices and ROC curves.\n",
    "\n",
    "**Test Set**\n",
    "\n",
    "The best model is used to predict the class of the images in the test dataset. This was then submitted to a Kaggle competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score,confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configurations for reproducabiity\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(path, transforms=torchvision.transforms.ToTensor()):\n",
    "    # Read the images\n",
    "    images = torchvision.datasets.ImageFolder(path, transform=transforms)\n",
    "    return images\n",
    "\n",
    "#images_dataset = read_images('/home/magsam/workspace/AGAR-hack-2024/data/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to display the images\n",
    "def display_images(images, num_images=10):\n",
    "    # Create a new figure\n",
    "    _, ax = plt.subplots(1, num_images, figsize=(20, 2))\n",
    "    # Loop through the images and display them\n",
    "    for i in range(num_images):\n",
    "        ax[i].imshow(images[i].permute(1, 2, 0))\n",
    "        ax[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Display the images\n",
    "#images_list = [images_dataset[i][0] for i in range(10)]\n",
    "#display_images(images_list, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindAndCropLargeCircle(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A class that finds and crops the largest circle in an image.\n",
    "\n",
    "    Args:\n",
    "        img (PIL.Image.Image): The input image.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: The cropped image containing the largest circle, or the original image if no suitable circle is found.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to numpy array\n",
    "        image = np.array(img)\n",
    "\n",
    "        # Resize the image to decrease resolution\n",
    "        new_height = image.shape[0] // 2\n",
    "        new_width = image.shape[1] // 2\n",
    "        resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "        # Convert the resized image to grayscale\n",
    "        gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        # Apply Hough Circle Transform to detect circles\n",
    "        circles = cv2.HoughCircles(blurred, cv2.HOUGH_GRADIENT, dp=1, minDist=50,\n",
    "                                   param1=250, param2=30, minRadius=100, maxRadius=2200)\n",
    "\n",
    "        # Ensure at least one circle was found\n",
    "        if circles is not None:\n",
    "            # Convert coordinates and radius to integers\n",
    "            circles = np.round(circles[0, :]).astype(\"int\")\n",
    "\n",
    "            # Loop over the circles\n",
    "            for (x, y, r) in circles:\n",
    "                # Check if the circle has a diameter at least half the height of the image\n",
    "                if r * 2 >= new_height / 2:\n",
    "                    # Crop the image based on the detected circle\n",
    "                    cropped_image = resized_image[y-r:y+r, x-r:x+r]\n",
    "\n",
    "                    # Convert cropped image back to PIL for torchvision compatibility\n",
    "                    cropped_image_pil = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "                    return cropped_image_pil\n",
    "\n",
    "        # If no suitable circle is found, or at the end of the function, return the original image\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train split\n",
    "- 80% of the images for training\n",
    "- 20% of the images for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stratified_indices(dataset_folder, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Generate stratified train and validation indices based on the labels of the images in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_folder (str): The path to the dataset folder.\n",
    "    - test_size (float): The proportion of the dataset to include in the validation set.\n",
    "\n",
    "    Returns:\n",
    "    - train_idx (list): The indices of the images to be used for training.\n",
    "    - val_idx (list): The indices of the images to be used for validation.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    img_files = [f for f in os.listdir(os.path.join(dataset_folder, \"train_data\")) if f.endswith(\".jpg\")]\n",
    "    for img_file in img_files:\n",
    "        json_path = os.path.join(dataset_folder, \"train_data\", os.path.splitext(img_file)[0] + \".json\")\n",
    "        with open(json_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "            labels.append(0 if metadata[\"colonies_number\"] == 0 else 1)\n",
    "            \n",
    "    # Generate stratified train and validation indices\n",
    "    train_idx, val_idx = train_test_split(range(len(img_files)), test_size=test_size, stratify=labels, random_state=42)\n",
    "    return train_idx, val_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for loading image and metadata pairs.\n",
    "\n",
    "    Args:\n",
    "        dataset_folder (str): Path to the dataset folder.\n",
    "        split (str, optional): Split of the dataset (e.g., \"train\", \"test\"). Defaults to \"train\".\n",
    "        transform (callable, optional): Optional transform to be applied to the image. Defaults to None.\n",
    "        indices (list, optional): List of indices to use from the dataset. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the image and the target label.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_folder, test=False, transform=None, indices=None):\n",
    "        self.split = \"test\" if test else \"train\"\n",
    "        self.data_path = os.path.join(dataset_folder, f\"{self.split}_data\")\n",
    "        self.transform = transform\n",
    "        self.img_files = [f for f in os.listdir(self.data_path) if f.endswith(\".jpg\")]\n",
    "        self.indices = indices if indices is not None else range(len(self.img_files))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[self.indices[idx]]\n",
    "        img_path = os.path.join(self.data_path, img_name)\n",
    "        json_name = os.path.splitext(img_name)[0] + \".json\"\n",
    "        json_path = os.path.join(self.data_path, json_name)\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.split == \"test\":\n",
    "            return image, torch.tensor([])\n",
    "        \n",
    "        with open(json_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "            num_colonies = metadata[\"colonies_number\"]\n",
    "            is_empty = 0 if num_colonies == 0 else 1\n",
    "        \n",
    "        return image, is_empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms for the training set\n",
    "train_transforms = v2.Compose([\n",
    "    v2.Resize((256, 256)),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomRotation(20),\n",
    "    v2.CenterCrop(224),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define the transforms for the validation set\n",
    "val_transforms = v2.Compose([\n",
    "    v2.Resize((256, 256)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/magsam/workspace/AGAR-hack-2024/data/raw/'\n",
    "train_idx, val_idx = generate_stratified_indices(path) #Get the indices for the train and validation sets\n",
    "\n",
    "# Create the train and validation datasets\n",
    "train_ts=CustomDataset(path, transform=train_transforms, indices=train_idx)\n",
    "val_ts=CustomDataset(path, transform=val_transforms, indices=val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8632, 2159)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ts), len(val_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) 1\n",
      "torch.Size([3, 224, 224]) 1\n",
      "torch.Size([3, 224, 224]) 1\n",
      "torch.Size([3, 224, 224]) 1\n",
      "torch.Size([3, 224, 224]) 1\n",
      "torch.Size([3, 224, 224]) 1\n",
      "torch.Size([3, 224, 224]) 0\n"
     ]
    }
   ],
   "source": [
    "# Check if our transformations was applied\n",
    "ii=-1\n",
    "for x,y in train_ts:\n",
    "    print(x.shape,y)\n",
    "    ii+=1\n",
    "    if(ii>5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DataLoader\n",
    "train_dl = DataLoader(train_ts,\n",
    "                      batch_size=32,\n",
    "                      num_workers=6, \n",
    "                      shuffle=True)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_dl = DataLoader(val_ts,\n",
    "                    batch_size=32,\n",
    "                    num_workers=6,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric\n",
    "def eval_metric(y_true, y_pred):\n",
    "   score = accuracy_score(y_true, y_pred) * recall_score(y_true, y_pred)\n",
    "   return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model \n",
    "from torchvision.models import densenet121\n",
    "\n",
    "model = densenet121(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Make the model last layer for binary classification\n",
    "num_features = model.classifier.in_features\n",
    "\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_features, 1),\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device (CPU or CUDA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the model architecture\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(32, 3, 224, 224), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/experiment_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, model, loss_function, optimizer, scheduler, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Trains a model using the provided data loaders, loss function, optimizer, and scheduler.\n",
    "\n",
    "    Args:\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation data.\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        loss_function (torch.nn.Module): The loss function used for training.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler): The scheduler used for adjusting the learning rate.\n",
    "        num_epochs (int, optional): Number of epochs to train the model (default is 25).\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    for epoch in trange(num_epochs, desc='Epochs'):\n",
    "        model.train()  \n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()  # Ensure labels are float for BCEWithLogitsLoss\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()  # Remove any additional dimensions\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}')\n",
    "        # Log training loss to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc='Validation', leave=False):\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float()  # Ensure labels are float for BCEWithLogitsLoss\n",
    "                outputs = model(inputs).squeeze()  # Remove any additional dimensions\n",
    "                loss = loss_function(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                preds = outputs > 0.0  # Apply threshold at 0 to logits to get predictions\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_score = eval_metric(y_true, y_pred)  # Make sure eval_metric is suitable for binary classification\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Score: {val_score:.4f}')\n",
    "\n",
    "        # Log validation loss and evaluation score to TensorBoard\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Score/val', val_score, epoch)\n",
    "\n",
    "        # Check if this is the best model so far and save\n",
    "        if val_score > best_score:\n",
    "            print(f'New best model found! Saving model from epoch {epoch+1}')\n",
    "            best_score = val_score\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), 'best_model_run2.pth')\n",
    "\n",
    "        scheduler.step(val_score)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_dl, val_dl, model, loss_function, optimizer, scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_dl, desc='Validation', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()  # Ensure labels are float for BCEWithLogitsLoss\n",
    "        outputs = model(inputs).squeeze()  # Remove any additional dimensions\n",
    "        preds = outputs > 0.0  # Apply threshold at 0 to logits to get predictions\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "val_score = eval_metric(y_true, y_pred) \n",
    "print(f'Validation Score: {val_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizulize the valdiation scores\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot confusion matrix\n",
    "axs[0].set_title('Confusion Matrix')\n",
    "ConfusionMatrixDisplay(cm).plot(ax=axs[0])\n",
    "\n",
    "# Plot ROC curve\n",
    "axs[1].set_title('ROC Curve')\n",
    "RocCurveDisplay.from_predictions(y_true, y_pred, ax=axs[1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_ts = CustomDataset(path, test=True, transform=val_transforms)\n",
    "\n",
    "print(len(test_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test DataLoader\n",
    "test_dl = DataLoader(test_ts,\n",
    "                    batch_size=32,\n",
    "                    num_workers=6,\n",
    "                    shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the infrence function\n",
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(test_loader, desc='Predicting', leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs).squeeze()  # Remove any additional dimensions\n",
    "            preds = (outputs > 0).long()  # Apply threshold at 0 to logits to get predictions\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "def extract_id(file_name):\n",
    "    return re.search(r'\\d+', file_name).group(0)\n",
    "\n",
    "# Extract the IDs from the test dataset\n",
    "test_ids = [extract_id(f) for f in test_ts.img_files]\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'ID': test_ids, 'TARGET': y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change id to int\n",
    "submission_df['ID'] = submission_df['ID'].astype(int)\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AGAR_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
